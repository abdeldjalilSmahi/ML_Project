{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b7f493",
   "metadata": {},
   "source": [
    "# Projet Machine Learnig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a654a",
   "metadata": {},
   "source": [
    "## Dataset UCI Support2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7872e",
   "metadata": {},
   "source": [
    "### Membre de groupe\n",
    "*    Abdeldjalil SMAHI : abdeldjalil.smahi@ens.uvsq.fr\n",
    "*    Oussama BOUAKAZ : oussama.bouakaz@ens.uvsq.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77918fea",
   "metadata": {},
   "source": [
    "## Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535cf896",
   "metadata": {},
   "source": [
    "N'exécutez pas ces deux cellulle si vous avez déj Xgboost et scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4e2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import psutil\n",
    "\n",
    "print(\"Nombre de CPU logiques :\", multiprocessing.cpu_count())\n",
    "print(\"Nombre de CPU physiques :\", psutil.cpu_count(logical=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e4ba2",
   "metadata": {},
   "source": [
    "### Configuration des affichages pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour afficher toutes les colonnes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Pour afficher toutes les lignes\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0d894",
   "metadata": {},
   "source": [
    "## Importation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb746f",
   "metadata": {},
   "source": [
    "Nous avons mis le dataset dans le repo github et puis l'importer directement du repo.\n",
    "\n",
    "\n",
    "Référence du dataset :\n",
    "\n",
    "\n",
    "*   [UCI Support2 Dataset](https://archive.ics.uci.edu/dataset/880/support2)\n",
    "*   [Description du dataset](https://hbiostat.org/data/repo/supportdesc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817f1d0",
   "metadata": {},
   "source": [
    "**SUPPORT 2 dataset** est une collection de données publié sur : [UCI Support2 Dataset](https://archive.ics.uci.edu/dataset/880/support2). Collecté entre *1989 – 1991 et 1992-1994* de **5 centres médicaux localisés aux USA**. Cette collection correspand aux données du **9105 patients** gravement malades telque chaque ligne correspond à un dossier du patient hospitalisé qui répond aux critères d’inclusion et d’éxclusion pour neuf catégories de maladies :\n",
    "*   insuffisance respiratoire aiguë\n",
    "*   maladie pulmonaire obstructive chronique\n",
    "*   insuffisance cardiaque congestive\n",
    "*   maladie du foie\n",
    "*   coma\n",
    "*   cancer du côlon\n",
    "*   cancer du poumon\n",
    "*   défaillance de plusieurs organes avec malignité\n",
    "*   défaillance de plusieurs organes avec sepsis1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a3953c",
   "metadata": {},
   "source": [
    "Le dataset est composé de **45** colonnes (attributs) dont 3 peuvent être utilisées comme targets (y’en a d’autres) et le **42** qui restent comme caractéristiques.\n",
    "Le code suivant affiche une sous ensemble du dataset (50 premiers ligne) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/abdeldjalilSmahi/ML_Project/main/support2_dataset.csv\"\n",
    "\n",
    "# Importation du dataset depuis l'URL\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Affichage des 50 premières lignes de notre dataset\n",
    "data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929c202",
   "metadata": {},
   "source": [
    "## Analyse Exploratoire des Données (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330f034",
   "metadata": {},
   "source": [
    "### Déscription du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd01223",
   "metadata": {},
   "source": [
    "Alors, comme indiqué précedement, le dataset est composé de 9105 ligne et 45 colonnes comme l'indique le code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18375ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cd3cc",
   "metadata": {},
   "source": [
    "Le code suivant donne des informations sur le dataset, nous allons expliqué par la suite chaque attribut en se basant sur la discription du site **UCI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999c7ba",
   "metadata": {},
   "source": [
    "**Explication de chaque colonne**\n",
    "\n",
    "*   **age:**  Cette variable, indique l'age du patient en *année* au moment de son entrée dans l'étude. Est une variable numérique et représente **une caractéristique (feature)** .\n",
    "*   **sex:** Sexe du patient. est une variable catégorielle qui peut prendre l'une des deux valeurs {male, female}.\n",
    "*   **dzclass:** La catégorie de la maladie du patient, une caractéristique catégorielle qui peut prendre l'une de ces valeurs : {ARF/MOSF\", \"COPD/CHF/Cirrhose\", \"Cancer\", \"Coma\"}.  telque *ARF/MOSF* indique l'insuffisance rénale aigue/syndrome de dysfonctionnement multiviscéral, *COPD/CHF/Cirrhose* indique que le patient atteint une Bronchopneumopathie obstructive chronique (BPCO)/insuffisance cardiaque congestive (ICC)/cirrhose du foie.\n",
    "*   **dzgroup:** C'est la sous catégorie de la maladie du patient, est une caracatérisitque catégorielle qui prend les valeurs : {ARF/MOSF w/Sepsis, CHF, COPD, Cirrhosis, Colon Cancer, Coma, Lung Cancer, MOSF w/Malig}: ARF/MOSF w/Sepsis indique : Insuffisance rénale aiguë (IRA)/syndrome de dysfonctionnement multiviscéral (MODS) avec sepsis. CHF indique : Insuffisance cardiaque congestive (ICC). COPD indique : Bronchopneumopathie obstructive chronique (BPCO). Cirrhosis inidique : Cirrhose du foie. Colon Cancer : Cancer du côlon et du rectum. Coma inidique : État de conscience altéré profond et prolongé. Lung Cancer indique : Cancer du poumon. MOSF w/Malig indique : Syndrome de dysfonctionnement multiviscéral (MODS) avec malignité.\n",
    "*   **num.co:**   Nombre de maladie simultannées ou comorbidités qui veut dire la présence d'une maladie supplémentaire qui coexiste avec la maladie principlae du patient. cette varibale prend des valeurs numérique, la plus élevée indique **un état de chances de survie plus mauvais**\n",
    "\n",
    "*   **edu:** nombre d'année d'études.\n",
    "*   **income** :  Revenu du patient annulle en k\\$. Cette variable est importante, elle nous donne une idée sur la possiblité de patient de rester hispotalisé, les patients ayant un niveau de revenu plus élevé sont moins susceptibles de décéder par rapport aux patients ayant un niveau de revenu plus faible.\n",
    "*   **scoma:** score de scoma du jour 3 basé sur l'échelle de Glasgow, la valeur est prédite par un modèle et est utilisé pour évaluer le niveau du conscience d'un patient en prenant en conisdération 3 critères l'ouverture des yeux, la réponse verbale et la réponse motrice. cette variable joue un role important car les patients ayant un score de coma plus bas sont les plus susceptibles de décéder.\n",
    "*   **charges:**  frais d’hospitalisation, elle indique les frais d’hospitalisation du patient.  \n",
    "*   **totcost :**  cout total du ratio couts/frais, cette variable indique le cout total du ration couts/frais du patient qui indique l’éfficacité des soins fournis au patient.\n",
    "*   **totmcst :** cette variable indique le cout total micro du patient, c’est cout estimatif total des soins fournis aux patient.\n",
    "*   **avtisst :** Score TISS moyen du patient sur les jours 3 à 25 de son hospitalisation, elle permet d’évaleur l’intensité des soins fournis au patient.\n",
    "*   **race :** La race du patient, variable catégorielle, qui peut prendre des valeurs {asian, black, hispanic, missing, other, white}.\n",
    "*    **sps** :  Score de physilogie du jour 3 du patient ele permet d’évaluer la gravité de sa maladie. cette valeur est prédite par un autre modèle.\n",
    "*    **aps** : Score de physiologie APCHE III du jour 3 sans coma, byn , uout. Elle permet d’évaluer la gravité de maladie d’un patient en USI ou en USIM.\n",
    "*   **surv2m :** les valeurs de cette colonne sont estimé par un autre modèle, elles indiquent l’estimation de la survie du patient à 2 mois au jour 3 de son hospitalisation. [](url) dans la documentation recommande d’ignorer cette colonne au moment de l’entrainement de notre modèle pour ne pas influencer son apprentissage.\n",
    "*    **surv6m :**  Comme la variable précedente, cette variable est prédite par un modèle elle indique l’estimation de la survie à 6 mois du patient au jour 3.\n",
    "*   **hday** : La variable hday, indique le jours d’hospitalisation auquel le patient est entré dans l’étude.\n",
    "*   **diabetes** :  cette variable indique si le patient est diabétique ou pas afin d’étudier l’impact de cette maladie sur le pronostic du patient. 0 si non, 1 s’il presente le diabète.\n",
    "*    **dementia:** indique si le patient présente de deux ou plusieurs maladies. 0 si non, 1 si oui .\n",
    "*  **ca :** cette variable indique si le patient a un cancer ou pas et s’il est propagé à d’autres partie du corps. Elle prend les valeurs suivantes {no, yes, metastatic}.  \n",
    "*   **prg2m :** represente l’estimation du médecin du taux de survie du patient à 2 mois en pourcentage (entre 0 et 1)\n",
    "*   **prg6m :** represente l’estimation du médecin du taux de survie du patient à 6 mois en pourcentage (entre 0 et 1)\n",
    "*   **dnr :** indique si le patient a ou non un ordre de ne pas réanimer. Elle prend les valeurs suivantes : {no dnr, dnr after sadm, dnr before sadm}\n",
    "*   **dnrday:** le jour de l’ordre dnr du patient <0 si l’ordre a été donné avant l étude.\n",
    "*   **meanbp :** cette variable indique la pression artérielle mpyenne du patient mesurée au jour 3.  Une valeur elevée  peut augmenter le risque de maladies cardiovasculaires.\n",
    "*   **wblc :** idnique le nombre de globules (en milliers) mesuré au jour 3.\n",
    "*  **hrt** : cette variable indique la fréquence cardiaque du patient mesurée au 3 eme jour, mesuré en nombre de battements par seconde.\n",
    "* **resp :** indique la fréquence réspiratoire du patient. mesuré en nombre de respirations par minute.\n",
    "*    **temp** :  cette variable indique la temparture corporelle en °C du patient.\n",
    "*   **pafi:** est une variable numérique qui indique le rapport PaO2/fiO2 (pression partielle d'oxygène dans le sang artériel par oxygène inspiré), un rapport normal devrait etre sup à 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30471225",
   "metadata": {},
   "source": [
    "### Distribution des types de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9a944f",
   "metadata": {},
   "source": [
    "Dans la problèmatique supervisé, et comme indiqué dans le catalogue du dataset qu'il existe 3 targets dans ce dataset. Nous allons considérer que la colonne **death** comme colonne target et faire une classification binaire la-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b60296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['hospdead', 'sfdm2'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70b7df",
   "metadata": {},
   "source": [
    "Le graphique circulaire ci-dessous, indique la distribution des types de colonnes dans le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8caaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bacf61",
   "metadata": {},
   "source": [
    "Plus précisément, nous avons : \n",
    "Types de variables :     \n",
    "*    **quantitatives** : 37\n",
    "*    **qualitatives** : 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fe740",
   "metadata": {},
   "source": [
    "### Analyse univariée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a279ee",
   "metadata": {},
   "source": [
    "#### Distribution des classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['death'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1acf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(df[['death']], return_counts=True)\n",
    "plt.title('Distribution des classes')\n",
    "sns.barplot(x=unique, y=counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a77e8",
   "metadata": {},
   "source": [
    "Nous déduisons que dans la problématique supervisé, nous sommes face à un problème de classification binaire dans lequel les classes ne sont pas équi-distribué, nous devons prendre ça en considération par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['death'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3088cb",
   "metadata": {},
   "source": [
    "Parmi les 9105 patients de cette étude, 68% entre eux ont décédé, et 32% entre eux ont survécu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060d58cd",
   "metadata": {},
   "source": [
    "##### Map des valeures nulles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014663e",
   "metadata": {},
   "source": [
    "Y'en a pas beaucoup de champs blancs, ce qui veut dire qu'il y en a très peu de valeurs manquantes. Le code suivant, va nous aider à calculer la proportion des valeurs manquantes et les trier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d26135",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isna().sum()/df.shape[0]).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d84d7",
   "metadata": {},
   "source": [
    "##### Analyse de chaque Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59680ebe",
   "metadata": {},
   "source": [
    "Séparation des Variables Numériques et Catégoriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_numeriques = df.select_dtypes(include=['int64', 'float64'])\n",
    "colonnes_categoriques = df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ae681",
   "metadata": {},
   "source": [
    "On analyse les variables numériques d'un ensemble de données. Il fournit des statistiques essentielles telles que la moyenne, l'écart type et les quartiles, montrant les tendances centrales et la dispersion des données. Les graphiques produits (histogramme et boîte à moustaches) offrent des vues visuelles de la distribution et des caractéristiques clés.\n",
    "Voici l'intrepretation de la fonction describe ():\n",
    "*  Count : Le nombre total d'éléments dans la variable.\n",
    "*  Mean : La moyenne, c'est-à-dire la valeur centrale des données.\n",
    "*  Std : L'écart type, indiquant à quel point les valeurs sont dispersées autour de la moyenne.\n",
    "*  Min : La plus petite valeur dans la variable.\n",
    "*  25% : Le premier quartile, représentant le point en-dessous duquel se trouvent 25% des données.\n",
    "*  50% : La médiane, divisant les données en deux moitiés égales.\n",
    "*  75% : Le troisième quartile, en-dessous duquel se situent 75% des données.\n",
    "*  Max : La plus grande valeur dans la variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_numeriques.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7308577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle sur chaque colonne numérique du dataset\n",
    "for column in colonnes_numeriques.columns:\n",
    "\n",
    "    summary_statistics = colonnes_numeriques[column].describe()\n",
    "    print(f\"Résumé statistique pour la variable '{column}':\")\n",
    "    print(summary_statistics)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(6, 4))\n",
    "\n",
    "    # Histogramme de la colonne\n",
    "    axes[0].hist(colonnes_numeriques[column], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[0].set_title(f'Histogramme pour {column}')\n",
    "    axes[0].set_xlabel(column)\n",
    "    axes[0].set_ylabel('Fréquence')\n",
    "\n",
    "    # Boîte à moustaches (boxplot) de la colonne\n",
    "    sns.boxplot(x=colonnes_numeriques[column], color='skyblue', ax=axes[1])\n",
    "    axes[1].set_title(f'Boîte à moustaches pour {column}')\n",
    "    axes[1].set_xlabel(column)\n",
    "\n",
    "    # Ajuster l'espacement entre les sous-graphes pour éviter les chevauchements\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Afficher les graphes\n",
    "    plt.show()\n",
    "    missing_values = colonnes_numeriques[column].isnull().sum()\n",
    "    print(f\"\\n Nombre de valeurs manquantes pour '{column}': {missing_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca416881",
   "metadata": {},
   "source": [
    "Nous appliquons maintenant la même méthode d'analyse aux variables catégoriques,on affiche le résumé statistique généré par describe(), montrant des statistiques clés comme le nombre total d'occurrences (count), la catégorie la plus fréquente (top) et son nombre d'occurrences (freq).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbd39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_categoriques.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes('object'):\n",
    "    print(f'{col :-<50} {df[col].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8935bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in colonnes_categoriques.columns:\n",
    "    print(f\"Résumé statistique pour la variable '{column}':\")\n",
    "    print(colonnes_categoriques[column].describe())\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 4))\n",
    "\n",
    "\n",
    "    sns.countplot(x=column, data=colonnes_categoriques)\n",
    "    axes.set_title(f'Diagramme en barres pour {column}')\n",
    "    axes.set_xlabel(column)\n",
    "    axes.set_ylabel('Fréquence')\n",
    "    # Ajustement l'espacement entre les sous-graphes pour éviter les chevauchements\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Affichage de graphe\n",
    "    plt.show()\n",
    "     # Affichage des valeurs manquantes pour la colonne\n",
    "    missing_values = colonnes_categoriques[column].isnull().sum()\n",
    "    print(f\"\\nNombre de valeurs manquantes pour '{column}': {missing_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fee0a",
   "metadata": {},
   "source": [
    "Pi plot pour les variables categoriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d6e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in colonnes_categoriques :\n",
    "    plt.figure()\n",
    "    df[col].value_counts().plot.pie(autopct=\"%.2f%%\")\n",
    "         # Affichage des valeurs manquantes pour la colonne\n",
    "    missing_values = data[col].isnull().sum()\n",
    "    print(f\"\\nNombre de valeurs manquantes pour '{col}': {missing_values}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efb413f",
   "metadata": {},
   "source": [
    "### Analyse bivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e7f60",
   "metadata": {},
   "source": [
    "#### Relation entre les variables et la target *death*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117620f1",
   "metadata": {},
   "source": [
    "Pour faciliter cette étude, on a deux groupe de patients, ceux qui ont décédé, et ceux qui ont survecu.\n",
    "\n",
    "On les sépare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12622f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "death_patients = df[df['death']== 1]\n",
    "survived_patients = df[df['death']== 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b453a",
   "metadata": {},
   "source": [
    "Nous allons montrer avec des graphes la relation de nos variables avec l'aatribut target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce56978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in colonnes_numeriques:\n",
    "    plt.figure()\n",
    "    sns.distplot(death_patients[col], label='death')\n",
    "    sns.distplot(survived_patients[col], label='survive')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdc39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in colonnes_categoriques :\n",
    "    plt.figure()\n",
    "    sns.heatmap(pd.crosstab(df['death'], df[col]), annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c1038",
   "metadata": {},
   "source": [
    "En analysant les résultats ci-dessus:\n",
    "*    Le nombre de décés des males est supérieurs que les females.\n",
    "*    ceux qui sont atteint par *ARF/MOSF w/Sepsis* Insuffisance rénale aiguë (IRA)/syndrome de dysfonctionnement multiviscéral (MOFS) avec sepsis sont plus susceptible de mourir<\n",
    "*   Ceux aui touche ;oins de 11K $ par an sont plus susceptible de mourir, est cela du peut etre de l'incapcité de payer les charges hospitalières et les soins médicaux.\n",
    "*   Ceux qui ont un cancer metastatic, càd qui s'est propagé dans le corps sont plus susceptible de mourir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e316b",
   "metadata": {},
   "source": [
    "#### La matrice de corrélation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de la matrice de corrélation\n",
    "plt.figure(figsize=(48, 48))\n",
    "sns.heatmap(colonnes_numeriques.corr(), annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Matrice de Corrélation entre Toutes les Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df33837",
   "metadata": {},
   "source": [
    "*    La matrice de corrélation nous indique que **adlsc** et **adls** sont fortement corrélés, et c'est normal car les deux pratiquement represente la meme information, l'indice des activités quotidiennes (ADL) du patient.\n",
    "*    **surv6m** et **surv2m** sont aussi fortement corrélé car est une estimation à vivre de 2 mois et l'autre à 6 mois, et le fait que la possibilité que le patient meurt dans 2 mois donc il sera décédé d'ici 6 mois, on rappelle que ces valeurs sont prédites par un modèle. \n",
    "*    **prg2m** et **prg6m** sont aussi fortement corrélé, comme les précédentes sauf que c'est une estimation des medecins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2af1f1",
   "metadata": {},
   "source": [
    "## Prétraitement / Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79296fe",
   "metadata": {},
   "source": [
    "Dans le cadre de notre problématique supervisé, les colonnes fortement corrélées peuvent nuire à la performance du modèle. Il est donc recommandé de les supprimer pour éviter le surapprentissage. Si deux variables sont fortement corrélées, cela signifie qu'elles varient de manière similaire. Dans le cadre de l'apprentissage supervisé, cela signifie que le modèle peut apprendre à prédire la variable cible en se basant sur l'une des variables corrélées. Cela peut entraîner le surapprentissage, ce qui signifie que le modèle s'adapte trop aux données d'entraînement et ne parvient pas à généraliser aux données de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b523e850",
   "metadata": {},
   "source": [
    "Nous avons dit que **adls** et **adlsc** sont fortement corrélé. Nous allons supprimer **adls** car il possède des valeurs manquantes contrairement à **adlsc** . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['adls'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd0f54",
   "metadata": {},
   "source": [
    "De plus, suite à notre lecture de la littérature sur le dataset [[Description du dataset](https://hbiostat.org/data/repo/supportdesc)] , il est recommandé, dans le cadre de la création d'un modèle, de ne pas inclure les colonnes **aps**, **sps**, **surv2m** et **surv6m** en tant que prédicteurs. Il est également probablement préférable de ne pas utiliser les colonnes **prg2m**, **prg6m**, **dnr**, **dnrday**. Par ailleurs, **prg2m** et deja fortement corrélé avec **prg6m**, et **dnrday** est fortement corrélé avec **totcst** et **totmcst** en plus **totcst** et **totmcst** sont fortement corrélé avec **charge** , raison pour laquelle nous supprimons les colonnes **prg2m**, **prg6m**, **dnrday**, **totcst** et **totmcst** [matrice de corrélation ci dessus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['aps', 'sps', 'surv2m', 'surv6m'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['prg2m', 'prg6m', 'dnrday', 'totcst', 'totmcst'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec94c45",
   "metadata": {},
   "source": [
    "Concernant les colonnes **dzclass** et **dzgroup**, deux variables catégorielles implicitement corrélées, l'examen du descriptif du dataset révèle que **dzgroup** représente la sous-catégorie de la maladie, tandis que **dzclass** représente la catégorie globale de la maladie. En d'autres termes, les deux variables font référence à la même information, mais **dzgroup** offre un niveau de détail plus élevé que **dzclass**. Par conséquent, nous conservons **dzgroup** à la place de **dzclass**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8093cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['dzclass'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3a6f7",
   "metadata": {},
   "source": [
    "### Remplissage des champs nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9af62",
   "metadata": {},
   "source": [
    "Maintenant, c'est le moment de s'occuper avec les valeurs manquantes.\n",
    "\n",
    "Y'en a plusieurs strategies pour traiter ce genre de situation des valeurs null:\n",
    "\n",
    "\n",
    "1.   **Suppression des lignes contenant les valeurs null** : Supprimer ces ligne, cependant cette stratégie n'est pas trop admirable surtout si le nombre de ces tuples est considérable par rapport à la taille de dataset et ça peut entraîner une perte d'informations importantes si les valeurs manquantes sont réparties dans plusieurs colonnes ou lignes.\n",
    "2.   **Remplir par des statistiques** : Remplaces les valeurs null par des statistiques telles que la moyenne, la médiane ou par la valeur la plus fréquente (mode) \"généralement pour les attributs de type numérique on opte par la stratégie de la moyenne mais tout depend\". cette stratégie est simple et rapide à mettre en oeuvre mais elle peut générer un bruit sur la dataset que nous voyons un épée à double tranchant pour éviter le surapprentissage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d65e3c",
   "metadata": {},
   "source": [
    "Nous allons opter pour la deuxième strategie à savoir le remplissage par des statisiques.\n",
    "Nous réccupérons les colonnes qui possèdent des valeurs manquantes dans une variables : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee61508",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df.columns[df.isnull().any()]\n",
    "null_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c69615",
   "metadata": {},
   "source": [
    "Nous utilisons le SimpleImputer en lui indiquant la stratégie de remplissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "income_imputer = SimpleImputer(missing_values= np.nan, strategy='most_frequent')\n",
    "scoma_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "charges_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "avtisst_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "race_imputer = SimpleImputer(missing_values= np.nan, strategy='most_frequent')\n",
    "dnr_imputer = SimpleImputer(missing_values= np.nan, strategy='most_frequent')\n",
    "meanbp_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "wblc_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "hrt_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "resp_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "temp_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "pafi_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "alb_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "bili_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "crea_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "sod_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "ph_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "glucose_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "bun_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "urine_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')\n",
    "adlp_imputer = SimpleImputer(missing_values= np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1afe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct= ColumnTransformer(transformers=[\n",
    "      ('edu_imputer', edu_imputer, ['edu']),\n",
    "      ('income_imputer', income_imputer, ['income']),\n",
    "      ('scoma_imputer', scoma_imputer, ['scoma']),\n",
    "      ('charges_imputer', charges_imputer, ['charges']),\n",
    "      ('avtisst_imputer', avtisst_imputer, ['avtisst']),\n",
    "      ('race_imputer', race_imputer, ['race']),\n",
    "      ('dnr_imputer', dnr_imputer, ['dnr']),\n",
    "      ('meanbp_imputer', meanbp_imputer, ['meanbp']),\n",
    "      ('wblc_imputer', wblc_imputer, ['wblc']),\n",
    "      ('hrt_imputer', hrt_imputer, ['hrt']),\n",
    "      ('resp_imputer', resp_imputer, ['resp']),\n",
    "      ('temp_imputer', temp_imputer, ['temp']),\n",
    "      ('pafi_imputer', pafi_imputer, ['pafi']),\n",
    "      ('alb_imputer', alb_imputer, ['alb']),\n",
    "      ('bili_imputer', bili_imputer, ['bili']),\n",
    "      ('crea_imputer', crea_imputer, ['crea']),\n",
    "      ('sod_imputer', sod_imputer, ['sod']),\n",
    "      ('ph_imputer', ph_imputer, ['ph']),\n",
    "      ('glucose_imputer', glucose_imputer, ['glucose']),\n",
    "      ('bun_imputer', bun_imputer, ['bun']),\n",
    "      ('urine_imputer', urine_imputer, ['urine']),\n",
    "      ('adlp_imputer', adlp_imputer, ['adlp'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre  les types de données d'origine : \n",
    "original_dtypes = df.dtypes\n",
    "df_transformed = pd.DataFrame(ct.fit_transform(df), columns=['edu', 'income', 'scoma', 'charges', 'avtisst', 'race','dnr',\n",
    "       'meanbp', 'wblc', 'hrt', 'resp', 'temp', 'pafi', 'alb', 'bili', 'crea',\n",
    "       'sod', 'ph', 'glucose', 'bun', 'urine', 'adlp'])\n",
    "for col in df_transformed.columns:\n",
    "    df_transformed[col] = df_transformed[col].astype(original_dtypes[col])\n",
    "\n",
    "df[['edu', 'income', 'scoma', 'charges', 'avtisst', 'race','dnr',\n",
    "       'meanbp', 'wblc', 'hrt', 'resp', 'temp', 'pafi', 'alb', 'bili', 'crea',\n",
    "       'sod', 'ph', 'glucose', 'bun', 'urine', 'adlp']] = df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_numeriques = df.select_dtypes(include=['int64', 'float64'])\n",
    "colonnes_categoriques = df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e65f82",
   "metadata": {},
   "source": [
    "### Encodage des variables catégoriques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dd7c1a",
   "metadata": {},
   "source": [
    "Nous allons encoder nos variables categoriques selon la nature de chacune si la variable posséde un ordre important entre ses valeurs nous utilisons ***ordinalEncoding*** si elle ne posséde pas un ordre entre ses valeurs nous optons pour le **oneHotEncoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cats_name= list(colonnes_categoriques.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa255ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cats_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c463b",
   "metadata": {},
   "source": [
    "Comme nous voyons dans le dataset, les valeurs da la colonne (incom) *revenue des patients* ont un ordre entre eux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db279fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cats_name.pop(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income'] = df['income'].replace({\n",
    "    'under $11k': 0,\n",
    "    '$11-$25k': 1,\n",
    "    '$25-$50k': 2,\n",
    "    '>$50k': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80da6da",
   "metadata": {},
   "source": [
    "Comme la variable sex prend que deux valeurs différentes, nous nous permettons de l'encoder en utilisant **OrdinalEncoding** . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2690dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OrdinalEncoder()\n",
    "df[['sex']] = encoder.fit_transform(df[['sex']]).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cats_name.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=col_cats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9532caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a9023f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(48, 48))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=.5)\n",
    "plt.title('Matrice de Corrélation entre Toutes les Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b152c1d",
   "metadata": {},
   "source": [
    "En analysant la matrice y'a pas de correlation entre nos variables sauf pour les variables qui appartiennet à la meme colonne de base avant l'utilisation de onehoteEncoding , ce qui est logique ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878706c4",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2c3fbe",
   "metadata": {},
   "source": [
    "La sélection des caractéristiques est un processus itératif qui vise à identifier un sous-ensemble de variables pertinentes à partir des caractéristiques du dataset. Dans notre cas nous avons procédé de plusieurs méthodes de selection des caractéristiques afin de réduire le nombre de features: \n",
    "\n",
    "**Analyse des Composantes Principales (PCA)**:Nous allons nous servir de cette méthode en prenant les coefficients des features qui représentent des valeurs numériques qui indiquent l'importance d'une variable dans la projection, pour ce faire nous allons considérer les deux premières composantes importantes et nous prenons les variables qui possèdent les coefficients les plus élevés.\n",
    "\n",
    "**Sélection de K-meilleures Variables** : Cette méthode consiste à choisir un nombre \\textbf{K} de features à prendre en considération, elle selectionne les K caractéristiques les plus informatives dans le dataset selon plusieurs critères dans notre cas nous avons choisi **la variance**.\n",
    "\n",
    "**Sélection en se basant sur un Classifier** : Nous entraînons un Classifier RandomForest sur nos donnés qui est capable d'identifier l'importance de chaque caractéristique pour prédire la cible, ensuite, nous utilisant **SelectFromModel** afin de selectionner les caractéristiques les plus pertinentes.\n",
    "\n",
    "A la fin de ces trois méthodes, nous faisons **une intersections** des caractéristiques afin d'avoir un ensemble de features plus restreints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75f6930",
   "metadata": {},
   "source": [
    "Ici on fait une copie de notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_y = dfs.death\n",
    "dfs_x = dfs.drop(['death'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeab80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_x.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48dab0",
   "metadata": {},
   "source": [
    "Nous allons dans un premier temps choisir les variables qui devraient être mises à l'échelle qui vont etre la variable ou y a des escarts entres leurs valeurs : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8715a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['age', 'edu', 'num.co', 'scoma', 'charges', 'avtisst','hday', 'meanbp', 'wblc', 'hrt', 'resp', 'temp', 'pafi', 'alb', 'bili', 'crea', 'sod', 'ph', 'glucose', 'bun', 'urine', 'adlp', 'adlsc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a507304",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "dfs_x[cols_to_scale] =  sc.fit_transform(dfs_x[cols_to_scale].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b8e52",
   "metadata": {},
   "source": [
    "### PCA (analyse des composantes principales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff35eeb",
   "metadata": {},
   "source": [
    "Nous allons utiliser pca comme une premiére approche qui va nous aider à selectionner nos variables pertinentes et à reduire les dimensions . Le code ci-dessous transforme les variables originales en composantes principales. Le code utilise la bibliothèque scikit-learn pour effectuer la PCA, puis affiche le pourcentage de variance expliquée par chaque composante principale dans un graphique à barres appelé \"Scree Plot\". Ce graphique aide à déterminer combien de composantes principales doivent être conservées pour représenter efficacement les données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(dfs_x)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "per_var = np.round(pca.explained_variance_ratio_,decimals=1)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "plt.subplots(nrows=1, ncols=1, figsize=(12, 12))\n",
    "plt.bar(x=range(1,len(pca.explained_variance_ratio_)+1), height=pca.explained_variance_ratio_, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc800f",
   "metadata": {},
   "source": [
    "pca.components_[i] contient les coefs attribués à nos variables pour la i éme composante , dans notre cas on va prendre les deux premiéres composantes.\n",
    "Ce code extrait et trie les composantes principales obtenues à partir de l'analyse en composantes principales (PCA) pour chaque colonne du DataFrame `dfs_x`. Les paires clé-valeur triées sont stockées dans une liste appelée `list_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_components = []\n",
    "for i in range(0,2):\n",
    "    result_dict = {}\n",
    "    for key, value in zip(dfs_x.columns, np.absolute(pca.components_[i])):\n",
    "        result_dict[key] = value\n",
    "    list_component = sorted(result_dict.items(), key=lambda x: x[1])\n",
    "    list_components.append(list_component)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b3e51",
   "metadata": {},
   "source": [
    "Le code ci-dessous compte le nombre d'occurrences des clés (noms de colonnes) dans les listes triées de paires clé-valeur stockées dans `list_components`. Il sélectionne les 33 clés les plus fréquentes parmi toutes les listes triées et les affiche. En résumé, il identifie les 33 noms de colonnes les plus importants à partir des composantes principales extraites par l'analyse en composantes principales (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_counts = Counter()\n",
    "for data_list in list_components:\n",
    "    sorted_data = sorted(data_list, key=lambda x: x[1], reverse=True)[:33]\n",
    "    key_counts.update(dict(sorted_data).keys())\n",
    "\n",
    "top_keys = [key for key, count in key_counts.most_common(33)]\n",
    "\n",
    "print(top_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03810b47",
   "metadata": {},
   "source": [
    "\n",
    "### Selection à partir du RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5dbd1f",
   "metadata": {},
   "source": [
    "Ce code utilise un modèle de forêt aléatoire pour sélectionner les caractéristiques les plus importantes à partir de nos données (dfs_x dfs_y). Il imprime ensuite les noms de ces caractéristiques sélectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52dba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "selector = SelectFromModel(rfc)\n",
    "selector.fit(dfs_x, dfs_y)\n",
    "\n",
    "feature_indices = selector.get_support()\n",
    "\n",
    "feature_names = [dfs_x.columns[i] for i in range(len(dfs_x.columns)) if feature_indices[i]]\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f98640",
   "metadata": {},
   "source": [
    "### Selection à partir du K-Best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36279894",
   "metadata": {},
   "source": [
    "Ce code applique la méthode de sélection de fonctionnalités SelectKBest avec k=33, ce qui signifie qu'il sélectionne les 33 meilleures caractéristiques en se basant sur leur score. Il utilise ensuite les indices des caractéristiques sélectionnées pour extraire les noms correspondants des colonnes du DataFrame dfs_x. Enfin, il imprime les noms des caractéristiques sélectionnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd848530",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest( k=33)\n",
    "selector.fit(dfs_x.values, dfs_y.values)\n",
    "feature_indices = selector.get_support()\n",
    "\n",
    "feature_names_ = [dfs_x.columns[i] for i in range(len(dfs_x.columns)) if feature_indices[i]]\n",
    "\n",
    "\n",
    "print(feature_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda50ce7",
   "metadata": {},
   "source": [
    "A fin de choisir les variables les plus pertinentes à partir de 3 methodes mentionnées ci-dessus , si une varibale presentes dans 2 ou 3 vecteurs on la prends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d83d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(dfs_x.columns) \n",
    "cols_to_select = []\n",
    "for col in cols : \n",
    "    v1 = col in top_keys\n",
    "    v2 = col in feature_names_\n",
    "    v3 = col in feature_names\n",
    "    listin = [v1, v2, v3]\n",
    "    if listin.count(True) >= 2:\n",
    "        cols_to_select.append(col)\n",
    "print(cols_to_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b92906",
   "metadata": {},
   "source": [
    "Après avoir executé les trois méthodes de selections de caractéristiques, nous avons décidé, de prendre les colonnes suivantes pour passer à l'étape de conception des modèles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b96458",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_select.extend(['death'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559fceb0",
   "metadata": {},
   "source": [
    "En examinant le descriptif du dataset, nous remarquons également que le niveau d'étude du patient (**edu**) ne semble pas être une variable significative pour prédire son pronostic de survie ou de décès, donc il est nécessaire de ne pas considérer la variable edu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776499f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_select.pop(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff4ed9a",
   "metadata": {},
   "source": [
    "On imprime les variables qu'on va travailler avec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6839fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf962489",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs[cols_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8596c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c039c0d",
   "metadata": {},
   "source": [
    "## Augmentation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944f210",
   "metadata": {},
   "source": [
    "Comme indiqué précédemment, notre ensemble de données présente un déséquilibre notable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5b0c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = dfs['death'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec821bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_count, class_0_count = dfs['death'].value_counts()\n",
    "class_1_count, class_0_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae139c80",
   "metadata": {},
   "source": [
    "Nous allons effectue un sur-échantillonnage de la classe  (`death` égal à 1) dans le DataFrame `dfs` en créant un échantillon aléatoire de la classe majoritaire (`death` égal à 0) de la même taille que la classe minoritaire. Ensuite, il concatène cet échantillon suréchantillonné avec la classe minoritaire pour obtenir un ensemble de données équilibré (`dfs_balanced`) en termes de classes. Enfin, le code imprime le décompte des classes dans le nouvel ensemble de données pour vérifier qu'il est équilibré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_0 = dfs[dfs['death'] == 0]\n",
    "class_0_oversampled = class_0.sample(class_1_count, replace=True)\n",
    "\n",
    "# Concatenatenation de la classe rééchantillée :  '>50K' et la classe dominante'<=50K' afin d'obtenir la dataset balanced.\n",
    "dfs_balanced = pd.concat([dfs[dfs['death'] == 1], class_0_oversampled], axis=0)\n",
    "\n",
    "# Vérification\n",
    "class_counts = dfs_balanced['death'].value_counts()\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763aa16b",
   "metadata": {},
   "source": [
    "### Standarisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dffa946",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcabb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dfs.pop('death').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc060b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = list(set(cols_to_scale).intersection(set(list(dfs.columns)))) \n",
    "cols_to_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646dbf76",
   "metadata": {},
   "source": [
    "On divise notre ensemble de données et etiquette en deux groupe (train et test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25,\n",
    "                                                    random_state = 1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af99f745",
   "metadata": {},
   "source": [
    "Création du transformateur de colonnes avec StandardScaler pour les colonnes à mettre à l'échelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('scale', StandardScaler(), cols_to_scale)\n",
    "], remainder='passthrough')\n",
    "\n",
    "\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_test = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270499a",
   "metadata": {},
   "source": [
    "## Apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987918ba",
   "metadata": {},
   "source": [
    "On a exécuté le GridSearch en utilisant Jupyter Notebook du cluster Paris Saclay  : https://jupyterhub.ijclab.in2p3.fr/ qui utilise : \n",
    "\n",
    "\n",
    "*   Nombre de CPU logiques : 240\n",
    "*   Nombre de CPU physiques : 240"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dade003",
   "metadata": {},
   "source": [
    "Nous allons opter pour des paramètres initiaux dans nos modèles supervisés, afin de mener une recherche exhaustive sur la grille (grid search) et d'obtenir les meilleurs parametres pour chaque modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db4bb3",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a3481",
   "metadata": {},
   "source": [
    "\n",
    "1. **`C`** : Ce paramètre régularise le modèle. Une valeur plus élevée de `C` signifie une régularisation plus faible, permettant au modèle de s'ajuster davantage aux données d'entraînement, ce qui peut potentiellement conduire à un surajustement. Une valeur plus faible favorise une régularisation plus forte, limitant la complexité du modèle pour éviter le surajustement.\n",
    "\n",
    "2. **`kernel`** : Ce paramètre détermine le type de fonction noyau utilisée dans le modèle. `'linear'` pour un noyau linéaire, `'rbf'` (Radial basis function) pour un noyau gaussien adapté aux données non linéaires.\n",
    "\n",
    "3. **`gamma`** : Le paramètre gamma influence la forme de la fonction noyau. Des valeurs plus élevées créent des frontières de décision plus complexes, adaptées aux données d'entraînement. Une valeur trop élevée peut entraîner un surajustement. Des valeurs plus faibles créent des frontières de décision plus simples.\n",
    "\n",
    "4. **`coef0`** : Ce paramètre est utilisé par certains noyaux, comme le noyau polynomial ou sigmoïde. Il affecte la sensibilité du modèle aux coefficients.\n",
    "\n",
    "En ajustant ces paramètres via une recherche exhaustive sur la grille, le modèle SVM peut être finement réglé pour s'adapter au mieux aux données spécifiques sur lesquelles il est entraîné.\n",
    "\n",
    "Cette partie du code peut prendre un certain temps pour s'exécuter, donc nous l'avons exécutée une fois et nous avons ensuite commenté le code pour éviter de devoir l'exécuter à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3568dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#param_grid_svm = {\n",
    " #   'C': [0.1, 1, 10],\n",
    " #  'kernel': ['linear', 'rbf'],\n",
    " #   'gamma': ['scale', 'auto', 0.0001,0.001,0.005,0.01,0.1, 1,10],\n",
    " #   'coef0': [0.0, 0.1, 0.5 ,0.558,1,5,10,50,100],\n",
    " #   'decision_function_shape': ['ovo', 'ovr']\n",
    "#}\n",
    "\n",
    "#svm_gridsearch = GridSearchCV(SVC(), param_grid_svm, n_jobs=12, cv=5, scoring='accuracy', verbose=100)\n",
    "\n",
    "#svm_hyper_params_models = svm_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "#best_params_svm = svm_gridsearch.best_params_\n",
    "\n",
    "#print(best_params_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ebb9c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params_svm = {'C': 1,\n",
    " 'coef0': 0.0,\n",
    " 'decision_function_shape': 'ovo',\n",
    " 'gamma': 10,\n",
    " 'kernel': 'rbf'}\n",
    "best_params_svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb390a87",
   "metadata": {},
   "source": [
    "Initialisation du modéle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dac9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(**best_params_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b1093",
   "metadata": {},
   "source": [
    "Entrainer le modéle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ed386",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b86a8",
   "metadata": {},
   "source": [
    "On doit calculer plusieurs métriques d'évaluation de la performance du modèle :\n",
    "\n",
    "1. **Précision** : La précision mesure le pourcentage de prédictions positives correctes parmi toutes les prédictions positives. Une précision élevée indique que le modèle a réussi à identifier avec précision les vrais positifs.\n",
    "\n",
    "2. **Rappel** : Le rappel (ou sensibilité) mesure le pourcentage de vrais positifs identifiés par le modèle parmi tous les vrais positifs présents dans les données. Un rappel élevé signifie que le modèle a identifié la plupart des vrais positifs.\n",
    "\n",
    "3. **Exactitude** : L'exactitude mesure le pourcentage total de prédictions correctes parmi toutes les prédictions. Elle indique la capacité globale du modèle à prédire correctement les classes.\n",
    "\n",
    "4. **F1-score** : Le F1-score est la moyenne harmonique de la précision et du rappel. Il fournit un équilibre entre la précision et le rappel, offrant ainsi une mesure globale de la performance du modèle.\n",
    "\n",
    "Ensuite, le code imprime ces métriques pour évaluer la qualité des prédictions du modèle SVM sur l'ensemble de données de test (`y_test`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Précision :\", precision)\n",
    "print(\"Rappel :\", recall)\n",
    "print(\"Exactitude :\", accuracy)\n",
    "print(\"F1-score :\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4d8f8",
   "metadata": {},
   "source": [
    "Le code ci-dessous va tracer la matrice de confusion pour mieux visualiser nos resultats en montrant Tp:True postif, Fp: False postif , TN: True negatif , FN : false negatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce16b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['survived', 'dead']\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Réponses réelles')\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64ddea",
   "metadata": {},
   "source": [
    "Un recap de toutes les métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cr = classification_report(y_test, y_pred)\n",
    "print(\"Rapport de classification :\\n\", cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb1044",
   "metadata": {},
   "source": [
    "\n",
    "Ce code génère une courbe ROC (Receiver Operating Characteristic) pour évaluer la performance d'un modèle de classification binaire. Il calcule les taux de faux positifs (FPR) et de vrais positifs (TPR) à partir des données de test (y_test et y_pred), puis trace la courbe ROC avec l'aire sous la courbe (AUC) pour mesurer l'efficacité du modèle dans la discrimination entre les classes positives et négatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Tracer la courbe ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Courbe ROC (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs (FPR)')\n",
    "plt.ylabel('Taux de vrais positifs (TPR)')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5521c58b",
   "metadata": {},
   "source": [
    "Employez la bibliothèque Scikit-plot pour visualiser la courbe d'apprentissage du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f141b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.estimators.plot_learning_curve(svm, X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02db581",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eacb038",
   "metadata": {},
   "source": [
    "Xgboost est un modele d'apprentissage supervisé qui se base sur le boosting , L'idée principale derrière le boosting est de combiner plusieurs modèles d'apprentissage faibles pour créer un modèle fort.\n",
    "Les paramètres initiaux comprennent des valeurs variées pour des hyperparamètres tels que le nombre d'estimateurs (n_estimators), le taux d'apprentissage (learning_rate), la profondeur maximale de l'arbre (max_depth), le poids minimal d'un enfant (min_child_weight), le terme de régularisation gamma (gamma), le taux de sous-échantillonnage des données (subsample), la fraction de caractéristiques à utiliser par arbre (colsample_bytree), l'alpha de régularisation L1 (reg_alpha), et l'alpha de régularisation L2 (reg_lambda).\n",
    "On va mettre en place dans ce code une recherche exaustive qui évalue différentes combinaisons de ces paramètres pour trouver celle qui optimise la précision du modèle XGBoost.\n",
    "\n",
    "Cette partie du code peut prendre un certain temps pour s'exécuter, donc nous l'avons exécutée une fois et avons ensuite commenté le code pour éviter de devoir l'exécuter à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f95d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Nous définissons l'espace de recherche\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300, 500],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'min_child_weight': [1, 3, 5],\n",
    "#     'gamma': [0.01, 0.05, 0.1],\n",
    "#     'subsample': [0.7, 0.8, 0.9],\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "#     'reg_alpha': [0, 0.01, 0.05],\n",
    "#     'reg_lambda': [0, 0.01, 0.05],\n",
    "# }\n",
    "\n",
    "# # Puis nous créons l'objet GridSearchCV\n",
    "# gridsearch = GridSearchCV(XGBClassifier(), param_grid, n_jobs=11, cv=5, scoring='accuracy', verbose=100)\n",
    "# # Entraînez le modèle\n",
    "# hyper_params_models = gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# # on enregistre les meilleurs paramètres\n",
    "# best_params = gridsearch.best_params_\n",
    "\n",
    "# # on affiche les meilleurs paramètres\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22fb13",
   "metadata": {},
   "source": [
    "Voici les meilleurs parmaetres qu'on a trouvé en utilisant GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'colsample_bytree': 0.7,\n",
    " 'gamma': 0.05,\n",
    " 'learning_rate': 0.1,\n",
    " 'max_depth': 7,\n",
    " 'min_child_weight': 1,\n",
    " 'n_estimators': 500,\n",
    " 'reg_alpha': 0.05,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 0.8,\n",
    "        \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0eb99",
   "metadata": {},
   "source": [
    "Instanciation et entrainement du modéle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2823864",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb =  XGBClassifier(**best_params,objective= 'binary:logistic',\n",
    " nthread=-1,\n",
    "\n",
    ")\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da050619",
   "metadata": {},
   "source": [
    "Evaluation du modele selon les metriques mentionnées dans la partie du SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d775e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Précision :\", precision)\n",
    "print(\"Rappel :\", recall)\n",
    "print(\"Exactitude :\", accuracy)\n",
    "print(\"F1-score :\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f572ae1",
   "metadata": {},
   "source": [
    "La matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['survived', 'dead']\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Réponses réelles')\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e3f6f",
   "metadata": {},
   "source": [
    "Tracer la courbe Roc en se servant des resultats de la matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Courbe ROC (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs (FPR)')\n",
    "plt.ylabel('Taux de vrais positifs (TPR)')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a7f81f",
   "metadata": {},
   "source": [
    "Utilisez scikitplot pour tracer la courbe d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skplt.estimators.plot_learning_curve(xgb, X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9438a",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2ed4a",
   "metadata": {},
   "source": [
    "La régression logistique est un algorithme de classification qui est utilisé pour résoudre des problèmes binaires (deux classes). Ses principaux paramètres dans ce contexte sont :\n",
    "\n",
    "1. **`penalty`** : Cela spécifie la norme utilisée dans la régularisation. \"l1\" utilise la régularisation L1 (Lasso) qui ajoute la valeur absolue des coefficients, tandis que \"l2\" utilise la régularisation L2 (Ridge) qui ajoute le carré des coefficients.\n",
    "\n",
    "2. **`C`** : Il contrôle l'inverse de la force de régularisation. Des valeurs plus petites spécifient une régularisation plus forte. La plage spécifiée ici (np.logspace(-4, 4, 10)) signifie que C variera de 0.0001 à 10000.\n",
    "\n",
    "3. **`solver`** : C'est l'optimiseur utilisé dans le problème d'optimisation. \"liblinear\" est adapté aux petites données.\n",
    "\n",
    "Le code utilise `GridSearchCV` pour rechercher les meilleures combinaisons de ces paramètres en termes de précision (scoring=\"accuracy\") sur un ensemble de validation croisée de 5 plis (`cv=5`). Les meilleures valeurs de ces paramètres sont stockées dans `best_params_lgR` après l'exécution de la recherche sur la grille.\n",
    "\n",
    "Cette partie du code peut prendre un certain temps pour s'exécuter, donc nous l'avons exécutée une fois et avons ensuite commenté le code pour éviter de devoir l'exécuter à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     \"penalty\": [\"l1\", \"l2\"],\n",
    "#     \"C\": np.logspace(-4, 4, 10),\n",
    "#      \"solver\": [\"liblinear\"]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(LogisticRegression(), param_grid, scoring=\"accuracy\", cv=5, verbose=100)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_params_lgR = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgR = {'C': 0.3593813663804626, 'penalty': 'l2', 'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83504a15",
   "metadata": {},
   "source": [
    "Entrainer le modéle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr =  LogisticRegression(**best_params_lgR)\n",
    "lgr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgr.predict(X_test)\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Précision :\", precision)\n",
    "print(\"Rappel :\", recall)\n",
    "print(\"Exactitude :\", accuracy)\n",
    "print(\"F1-score :\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07041f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "labels = ['survived', 'dead']\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=labels, columns=labels)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Réponses réelles')\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31731eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "# Tracer la courbe ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Courbe ROC (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('Taux de faux positifs (FPR)')\n",
    "plt.ylabel('Taux de vrais positifs (TPR)')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.estimators.plot_learning_curve(lgr, X_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074da546",
   "metadata": {},
   "source": [
    "### Comparaison entre nos trois modéles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433eef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modeles = ['SVM', 'XGBOOST', 'LR']\n",
    "precision = [0.8334228909188608, 0.912761355443403, 0.7839013778100072] \n",
    "rappel = [1.0, 0.816247582205029, 0.696969696969697] \n",
    "F1score = [0.9091441969519344, 0.8618107556160652, 0.7378839590443687]     \n",
    "data = {'Modèle': modeles, 'Précision': precision, 'Rappel': rappel,'F1-Score' : F1score}\n",
    "df = pd.DataFrame(data)\n",
    "largeur_barre = 0.25\n",
    "indices = np.arange(len(modeles))\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(indices - largeur_barre, precision, largeur_barre, label='Précision', color='#B3E5FC')\n",
    "plt.bar(indices + largeur_barre, rappel, largeur_barre, label='Rappel', color='#75E6DA')\n",
    "plt.bar(indices , F1score, largeur_barre, label='f1-score', color='#CCD9E5')\n",
    "plt.xlabel('Modèles')\n",
    "plt.ylabel('Performances')\n",
    "plt.title('Comparaison des Modèles (Précision ,Rappel et F1-Score)')\n",
    "plt.xticks(indices, modeles)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c453c",
   "metadata": {},
   "source": [
    "Apres le tracage du graphe ci-dessus nous allons intrepreter les resultats :\n",
    "\n",
    "1. **SVM (Support Vector Machine)** :\n",
    "   - **Précision :** 83.34%\n",
    "   - **Rappel :** 100.0%\n",
    "   - **F1-score :** 90.91%\n",
    "\n",
    "   Le modèle SVM a une excellente sensibilité (rappel de 100%), ce qui signifie qu'il a réussi à identifier toutes les vraies valeurs positives dans le jeu de données. Cependant, cela vient au prix d'une précision légèrement inférieure, indiquant qu'il a également généré un certain nombre de faux positifs. Le score F1 de 90.91% équilibre précision et rappel, montrant une bonne performance globale.\n",
    "2. **XGBoost** :\n",
    "   - **Précision :** 91.28%\n",
    "   - **Rappel :** 81.62%\n",
    "   - **F1-score :** 86.18%\n",
    "\n",
    "   XGBoost affiche la meilleure précision parmi les trois modèles, indiquant qu'il fait des prédictions positives correctes dans 91.28% des cas. Cependant, son rappel est légèrement inférieur, suggérant qu'il manque certaines vraies valeurs positives. Le score F1 de 86.18% est un bon compromis entre précision et rappel.\n",
    "3. **Régression Logistique (LR)** :\n",
    "   - **Précision :** 78.39%\n",
    "   - **Rappel :** 69.70%\n",
    "   - **F1-score :** 73.79%\n",
    "\n",
    "   La régression logistique a des performances plus modestes en termes de précision et de rappel par rapport aux autres modèles. Cependant, elle offre un équilibre raisonnable entre précision et rappel, comme en témoigne son score F1 de 73.79%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410c5652",
   "metadata": {},
   "source": [
    "## Apprentissage non supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c995f",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25734ff9",
   "metadata": {},
   "source": [
    "L'objectif principal de K-means est de diviser un ensemble de données en K clusters, où K est un nombre prédéfini. L'algorithme fonctionne en assignant chaque donnée au cluster dont le centroïde (le point central) est le plus proche en termes de distance euclidienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b4456",
   "metadata": {},
   "source": [
    "Standarisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f04686",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "    ('scale', StandardScaler(), cols_to_scale)\n",
    "], remainder='passthrough')\n",
    "X = ct.fit_transform(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ec22b",
   "metadata": {},
   "source": [
    "Nous allons déterminer le nombre optimal de clusters pour nos données en utilisant deux méthodes différentes : la méthode du coude et la méthode de silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for k in range(1, 11):  \n",
    "    kmeans = KMeans(n_clusters=k,init = 'k-means++')\n",
    "    kmeans.fit(X)  \n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertias, marker='o', linestyle='--')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('Inertie')\n",
    "plt.title('Méthode du Coude pour trouver le meilleur K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2acaa",
   "metadata": {},
   "source": [
    "Pour le graphe ci-dessus, nous cherchons le point où l'ajout d'un nouveau cluster n'entraîne pas une réduction significative de la variation intra-cluster. En analysant le graphique, nous observons que la descente du graphe devient moins abrupte à partir du nombre de clusters égal à 3. Par conséquent, selon la méthode du coude, le meilleur nombre de clusters possible pour nos données est 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores_df = pd.DataFrame(columns=['Nombre de Clusters', 'Silhouette Score'])\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    new_row = pd.DataFrame({'Nombre de Clusters': [k], 'Silhouette Score': [silhouette_avg]})\n",
    "    silhouette_scores_df = pd.concat([silhouette_scores_df, new_row], ignore_index=True)\n",
    "heatmap_data = silhouette_scores_df.pivot_table(index='Nombre de Clusters', values='Silhouette Score')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt=\".3f\", linewidths=.5)\n",
    "plt.title('Scores de Silhouette pour Différents Nombres de Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c2525",
   "metadata": {},
   "source": [
    "La methode de silhouette montre que le meuilleur nombre de cluster pour nos données c'est 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56dabe",
   "metadata": {},
   "source": [
    "Nous allons utiliser l'analyse en composantes principales (PCA) pour réduire la dimension de nos données et ainsi visualiser nos clusters de manière plus efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee52959",
   "metadata": {},
   "source": [
    "K = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61532c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA()  \n",
    "principal_components_full = pca_full.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=2,init = 'k-means++') \n",
    "labels_2 = kmeans.fit_predict(principal_components_full)\n",
    "\n",
    "plt.scatter(principal_components_full[:, 0], principal_components_full[:, 1], c=labels_2, cmap='viridis')\n",
    "plt.xlabel('Composante Principale 1')\n",
    "plt.ylabel('Composante Principale 2')\n",
    "plt.title('Visualisation des Clusters avec K-means (Caractéristiques d\\'Origine)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ae43b",
   "metadata": {},
   "source": [
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA()  \n",
    "principal_components_full = pca_full.fit_transform(X)\n",
    "kmeans = KMeans(n_clusters=3,init = 'k-means++') \n",
    "labels_3 = kmeans.fit_predict(principal_components_full)\n",
    "plt.scatter(principal_components_full[:, 0], principal_components_full[:, 1], c=labels_3, cmap='viridis')\n",
    "plt.xlabel('Composante Principale 1')\n",
    "plt.ylabel('Composante Principale 2')\n",
    "plt.title('Visualisation des Clusters avec K-means (Caractéristiques d\\'Origine)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22640821",
   "metadata": {},
   "source": [
    "Les clusters sont bien determinés par les deux choix de nombres de clusters , Cependant on voit que y a un clustring un peu meilleur quand on utilise k= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb225b9e",
   "metadata": {},
   "source": [
    "Vu que notre probleme initial est un probléme de classification binaire nous allons tester le score 'jaccard_similarity' lorsque le k = 2 et nous comparons les labels de kmeans avec nos etiquettes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb87eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity = jaccard_score(y, labels_2, average='weighted')\n",
    "print(f\"Coefficient de Similarité de Jaccard : {jaccard_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e887e292",
   "metadata": {},
   "source": [
    "On remarque que le score de similarité est un peu faible , on déduit que nos données de base ne sont bien regroubé par l'algorithme k-means vu notre probléme principal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ac295",
   "metadata": {},
   "source": [
    "### L'algorithme héarchique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0083628d",
   "metadata": {},
   "source": [
    "L'algorithme hiérarchique est une méthode de regroupement non supervisée.Son objectif principal est de diviser un ensemble de données en clusters de manière progressive et itérative. L'algorithme commence par considérer chaque point de données comme un cluster individuel, puis fusionne progressivement les clusters voisins en fonction de leur similarité, formant ainsi des clusters plus grands.\n",
    "\n",
    "Contrairement à certains autres algorithmes de clustering, l'algorithme hiérarchique ne nécessite pas de spécifier le nombre de clusters à l'avance. Il permet d'explorer différentes configurations de clusters en fusionnant progressivement les clusters voisins à divers niveaux\n",
    "\n",
    "Le paramètre initial clé pour optimiser l'algorithme hiérarchique est la méthode de liaison (linkage method). La méthode de liaison détermine comment mesurer la distance ou la similarité entre les clusters lors de leur fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3645b2",
   "metadata": {},
   "source": [
    "Pour determiner le meilleur parametre pour optimiser cet algorithme , nous allons implémenter un algorithme qui va detrminer la meilleur méthode possible pour le linkage et la methode silhouette pour s'assurer du résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA()  \n",
    "principal_components_full = pca_full.fit_transform(X)\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, linkage_method in enumerate(linkage_methods, 1):\n",
    "    model = AgglomerativeClustering(linkage=linkage_method)\n",
    "    labels = model.fit_predict(principal_components_full)\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(principal_components_full[:, 0], principal_components_full[:, 1], c=labels, cmap='viridis')\n",
    "    plt.title(f'Linkage Method: {linkage_method.capitalize()}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a670c66",
   "metadata": {},
   "source": [
    "D'apres les resultats ci-dessus la meilleur methode de linkage à utiliser pour bien regrouper notre données c'est la méthode : ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a142ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "silhouette_scores_df_hier = pd.DataFrame(columns=['Nombre de Clusters', 'Silhouette Score'])\n",
    "\n",
    "for k in range(2, 11):\n",
    "    model = AgglomerativeClustering(n_clusters=k) \n",
    "    labels = model.fit_predict(X)\n",
    "    silhouette_avg = silhouette_score(X, labels)\n",
    "    new_row = pd.DataFrame({'Nombre de Clusters': [k], 'Silhouette Score': [silhouette_avg]})\n",
    "    silhouette_scores_df_hier = pd.concat([silhouette_scores_df_hier, new_row], ignore_index=True)\n",
    "heatmap_data = silhouette_scores_df_hier.pivot_table(index='Nombre de Clusters', values='Silhouette Score')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt=\".3f\", linewidths=.5)\n",
    "plt.title('Scores de Silhouette pour Différents Nombres de Clusters')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62f0ad",
   "metadata": {},
   "source": [
    "Comme montré ci-dessus le meilleur nombre de cluster possible c'est lorsque k=2 , et ça correspond trés bien au résultat trouvé par l'algorithme ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9054dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering() \n",
    "cluster_labels = model.fit_predict(X)\n",
    "jaccard_similarity = jaccard_score(y, labels, average='weighted')\n",
    "print(f\"Coefficient de Similarité de Jaccard : {jaccard_similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607104b",
   "metadata": {},
   "source": [
    "Le score est faible par rapport à l'algorithme k-means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
